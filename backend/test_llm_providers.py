"""
LLM Provideræµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯å„ä¸ªproviderçš„åŸºæœ¬åŠŸèƒ½
"""
import asyncio
import os
from app.core.llm_factory import get_provider
from app.core.llm_config import LLMProviderType
from app.core.llm_provider import ChatRequest, Message


async def test_openai():
    """æµ‹è¯•OpenAI Provider"""
    print("=" * 50)
    print("æµ‹è¯• OpenAI Provider")
    print("=" * 50)
    
    # æ£€æŸ¥API key
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸  OPENAI_API_KEYæœªè®¾ç½®ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    try:
        provider = get_provider(LLMProviderType.OPENAI)
        
        # æµ‹è¯•éæµå¼å¯¹è¯
        print("\n1. æµ‹è¯•éæµå¼å¯¹è¯...")
        request = ChatRequest(
            messages=[
                Message(role="user", content="ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½")
            ],
            temperature=0.7,
            max_tokens=100
        )
        
        response = await provider.chat(request)
        print(f"âœ… å“åº”: {response.content}")
        print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage}")
        
        # æµ‹è¯•æµå¼å¯¹è¯
        print("\n2. æµ‹è¯•æµå¼å¯¹è¯...")
        print("âœ… å“åº”: ", end="", flush=True)
        async for chunk in provider.chat_stream(request):
            print(chunk, end="", flush=True)
        print()
        
        # æµ‹è¯•tokenè®¡æ•°
        print("\n3. æµ‹è¯•tokenè®¡æ•°...")
        token_count = await provider.count_tokens("Hello, world!", "gpt-3.5-turbo")
        print(f"âœ… Tokenæ•°é‡: {token_count}")
        
        # è·å–å¯ç”¨æ¨¡å‹
        print("\n4. è·å–å¯ç”¨æ¨¡å‹...")
        models = provider.get_available_models()
        print(f"âœ… å¯ç”¨æ¨¡å‹: {', '.join(models)}")
        
        print("\nâœ… OpenAI Provideræµ‹è¯•é€šè¿‡ï¼\n")
        
    except Exception as e:
        print(f"\nâŒ OpenAI Provideræµ‹è¯•å¤±è´¥: {str(e)}\n")


async def test_anthropic():
    """æµ‹è¯•Anthropic Provider"""
    print("=" * 50)
    print("æµ‹è¯• Anthropic Provider")
    print("=" * 50)
    
    # æ£€æŸ¥API key
    if not os.getenv("ANTHROPIC_API_KEY"):
        print("âš ï¸  ANTHROPIC_API_KEYæœªè®¾ç½®ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    try:
        provider = get_provider(LLMProviderType.ANTHROPIC)
        
        # æµ‹è¯•éæµå¼å¯¹è¯
        print("\n1. æµ‹è¯•éæµå¼å¯¹è¯...")
        request = ChatRequest(
            messages=[
                Message(role="user", content="ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½")
            ],
            temperature=0.7,
            max_tokens=100
        )
        
        response = await provider.chat(request)
        print(f"âœ… å“åº”: {response.content}")
        print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage}")
        
        # æµ‹è¯•æµå¼å¯¹è¯
        print("\n2. æµ‹è¯•æµå¼å¯¹è¯...")
        print("âœ… å“åº”: ", end="", flush=True)
        async for chunk in provider.chat_stream(request):
            print(chunk, end="", flush=True)
        print()
        
        # è·å–å¯ç”¨æ¨¡å‹
        print("\n3. è·å–å¯ç”¨æ¨¡å‹...")
        models = provider.get_available_models()
        print(f"âœ… å¯ç”¨æ¨¡å‹: {', '.join(models)}")
        
        print("\nâœ… Anthropic Provideræµ‹è¯•é€šè¿‡ï¼\n")
        
    except Exception as e:
        print(f"\nâŒ Anthropic Provideræµ‹è¯•å¤±è´¥: {str(e)}\n")


async def test_ollama():
    """æµ‹è¯•Ollama Provider"""
    print("=" * 50)
    print("æµ‹è¯• Ollama Provider")
    print("=" * 50)
    
    try:
        provider = get_provider(LLMProviderType.OLLAMA)
        
        # æµ‹è¯•éæµå¼å¯¹è¯
        print("\n1. æµ‹è¯•éæµå¼å¯¹è¯...")
        request = ChatRequest(
            messages=[
                Message(role="user", content="Say hello in one sentence")
            ],
            temperature=0.7,
            max_tokens=50
        )
        
        response = await provider.chat(request)
        print(f"âœ… å“åº”: {response.content}")
        print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage}")
        
        # æµ‹è¯•æµå¼å¯¹è¯
        print("\n2. æµ‹è¯•æµå¼å¯¹è¯...")
        print("âœ… å“åº”: ", end="", flush=True)
        async for chunk in provider.chat_stream(request):
            print(chunk, end="", flush=True)
        print()
        
        # è·å–å¯ç”¨æ¨¡å‹
        print("\n3. è·å–å¯ç”¨æ¨¡å‹...")
        models = provider.get_available_models()
        print(f"âœ… å¯ç”¨æ¨¡å‹: {', '.join(models)}")
        
        print("\nâœ… Ollama Provideræµ‹è¯•é€šè¿‡ï¼\n")
        
    except Exception as e:
        print(f"\nâŒ Ollama Provideræµ‹è¯•å¤±è´¥: {str(e)}\n")
        print("ğŸ’¡ æç¤º: ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œ (ollama serve)")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹LLM Provideræµ‹è¯•...\n")
    
    # æµ‹è¯•å„ä¸ªprovider
    await test_openai()
    await test_anthropic()
    await test_ollama()
    
    print("=" * 50)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 50)


if __name__ == "__main__":
    asyncio.run(main())
