"""
RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•å‘é‡æ£€ç´¢ã€æ–‡æ¡£å¤„ç†ã€ç¼“å­˜ç­‰æ€§èƒ½æŒ‡æ ‡
"""
import time
import asyncio
from pathlib import Path
import statistics

from app.rag import get_knowledge_base, RetrievalMode
from app.rag.embedding_service import get_embedding_service, EmbeddingProvider
from app.utils.cache import get_search_cache


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.kb_name = "benchmark_kb"
        self.kb = None
        self.results = {}
    
    async def setup(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        print("\n" + "="*60)
        print("RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*60)
        
        # ä½¿ç”¨æœ¬åœ°embeddingä»¥è·å¾—ä¸€è‡´çš„æµ‹è¯•ç¯å¢ƒ
        print("\n[1] åˆå§‹åŒ–embeddingæœåŠ¡...")
        get_embedding_service(provider=EmbeddingProvider.LOCAL)
        print("âœ“ ä½¿ç”¨æœ¬åœ°embeddingæ¨¡å‹")
        
        # åˆ›å»ºæµ‹è¯•çŸ¥è¯†åº“
        print("\n[2] åˆ›å»ºæµ‹è¯•çŸ¥è¯†åº“...")
        self.kb = get_knowledge_base(
            self.kb_name,
            "æ€§èƒ½æµ‹è¯•çŸ¥è¯†åº“",
            chunk_size=500,
            chunk_overlap=50
        )
        print(f"âœ“ çŸ¥è¯†åº“åˆ›å»ºå®Œæˆ: {self.kb_name}")
    
    async def benchmark_document_processing(self, num_docs=100):
        """
        æµ‹è¯•æ–‡æ¡£å¤„ç†æ€§èƒ½
        
        Args:
            num_docs: æµ‹è¯•æ–‡æ¡£æ•°é‡
        """
        print(f"\n[3] æµ‹è¯•æ–‡æ¡£å¤„ç†æ€§èƒ½ (n={num_docs})...")
        
        # ç”Ÿæˆæµ‹è¯•æ–‡æ¡£
        test_docs = [
            f"è¿™æ˜¯æµ‹è¯•æ–‡æ¡£ {i}ã€‚å†…å®¹åŒ…å«äº†å…³äºäººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ çš„ç›¸å…³çŸ¥è¯†ã€‚"
            f"Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¿æ³›åº”ç”¨äºæ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ é¢†åŸŸã€‚"
            f"æ–‡æ¡£ID: {i}" * 5  # é‡å¤ä»¥å¢åŠ é•¿åº¦
            for i in range(num_docs)
        ]
        
        # æµ‹è¯•æ·»åŠ æ–‡æ¡£
        times = []
        for i, doc in enumerate(test_docs):
            start = time.time()
            await self.kb.add_text(
                doc,
                metadata={"doc_id": i, "category": "test"}
            )
            elapsed = time.time() - start
            times.append(elapsed)
            
            if (i + 1) % 20 == 0:
                print(f"  â€¢ å·²å¤„ç† {i+1}/{num_docs} ä¸ªæ–‡æ¡£")
        
        # ç»Ÿè®¡ç»“æœ
        avg_time = statistics.mean(times)
        median_time = statistics.median(times)
        
        self.results['document_processing'] = {
            'total_docs': num_docs,
            'avg_time_per_doc': avg_time,
            'median_time': median_time,
            'total_time': sum(times),
            'docs_per_second': 1 / avg_time
        }
        
        print(f"\n  âœ“ æ–‡æ¡£å¤„ç†å®Œæˆ:")
        print(f"    - æ€»æ–‡æ¡£æ•°: {num_docs}")
        print(f"    - å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’/æ–‡æ¡£")
        print(f"    - ä¸­ä½æ•°: {median_time:.3f}ç§’")
        print(f"    - å¤„ç†é€Ÿåº¦: {1/avg_time:.2f} æ–‡æ¡£/ç§’")
    
    async def benchmark_search_performance(self, num_queries=50):
        """
        æµ‹è¯•æœç´¢æ€§èƒ½
        
        Args:
            num_queries: æµ‹è¯•æŸ¥è¯¢æ•°é‡
        """
        print(f"\n[4] æµ‹è¯•æœç´¢æ€§èƒ½ (n={num_queries})...")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            "äººå·¥æ™ºèƒ½æœºå™¨å­¦ä¹ ",
            "Pythonç¼–ç¨‹è¯­è¨€",
            "æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œ",
            "æ•°æ®ç§‘å­¦åˆ†æ",
            "è‡ªç„¶è¯­è¨€å¤„ç†"
        ] * (num_queries // 5)
        
        modes = {
            "semantic": RetrievalMode.SEMANTIC,
            "keyword": RetrievalMode.KEYWORD,
            "hybrid": RetrievalMode.HYBRID,
            "rerank": RetrievalMode.RERANK
        }
        
        mode_results = {}
        
        for mode_name, mode in modes.items():
            print(f"\n  æµ‹è¯• {mode_name} æ¨¡å¼...")
            times = []
            
            for i, query in enumerate(test_queries):
                start = time.time()
                results = await self.kb.search(
                    query,
                    mode=mode,
                    k=5
                )
                elapsed = time.time() - start
                times.append(elapsed)
                
                if (i + 1) % 10 == 0:
                    print(f"    â€¢ å·²å®Œæˆ {i+1}/{num_queries} ä¸ªæŸ¥è¯¢")
            
            avg_time = statistics.mean(times)
            mode_results[mode_name] = {
                'avg_time': avg_time,
                'median_time': statistics.median(times),
                'min_time': min(times),
                'max_time': max(times),
                'queries_per_second': 1 / avg_time
            }
            
            print(f"    âœ“ å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’")
            print(f"    âœ“ æŸ¥è¯¢é€Ÿåº¦: {1/avg_time:.2f} æŸ¥è¯¢/ç§’")
        
        self.results['search_performance'] = mode_results
    
    async def benchmark_cache_performance(self, num_queries=100):
        """
        æµ‹è¯•ç¼“å­˜æ€§èƒ½
        
        Args:
            num_queries: æµ‹è¯•æŸ¥è¯¢æ•°é‡
        """
        print(f"\n[5] æµ‹è¯•ç¼“å­˜æ€§èƒ½ (n={num_queries})...")
        
        cache = get_search_cache()
        cache.clear()  # æ¸…ç©ºç¼“å­˜
        
        query = "æµ‹è¯•ç¼“å­˜æŸ¥è¯¢"
        
        # ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼ˆæ— ç¼“å­˜ï¼‰
        print("\n  ç¬¬ä¸€æ¬¡æŸ¥è¯¢ï¼ˆå†·å¯åŠ¨ï¼‰...")
        cold_times = []
        for i in range(10):
            start = time.time()
            await self.kb.search(query, k=5)
            cold_times.append(time.time() - start)
        
        avg_cold = statistics.mean(cold_times)
        print(f"    âœ“ å¹³å‡è€—æ—¶: {avg_cold:.3f}ç§’")
        
        # ç¬¬äºŒæ¬¡æŸ¥è¯¢ï¼ˆæœ‰ç¼“å­˜ï¼‰
        print("\n  é‡å¤æŸ¥è¯¢ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰...")
        hot_times = []
        for i in range(num_queries):
            start = time.time()
            await self.kb.search(query, k=5)
            hot_times.append(time.time() - start)
        
        avg_hot = statistics.mean(hot_times)
        speedup = avg_cold / avg_hot
        
        print(f"    âœ“ å¹³å‡è€—æ—¶: {avg_hot:.3f}ç§’")
        print(f"    âœ“ åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        self.results['cache_performance'] = {
            'cold_start_time': avg_cold,
            'cached_time': avg_hot,
            'speedup': speedup,
            'cache_hit_rate': 100.0
        }
    
    async def benchmark_concurrent_load(self, num_concurrent=10):
        """
        æµ‹è¯•å¹¶å‘è´Ÿè½½
        
        Args:
            num_concurrent: å¹¶å‘æŸ¥è¯¢æ•°
        """
        print(f"\n[6] æµ‹è¯•å¹¶å‘è´Ÿè½½ (å¹¶å‘æ•°={num_concurrent})...")
        
        query = "å¹¶å‘æµ‹è¯•æŸ¥è¯¢"
        
        # å¹¶å‘æŸ¥è¯¢
        async def run_query():
            start = time.time()
            await self.kb.search(query, k=5)
            return time.time() - start
        
        print(f"  å¯åŠ¨ {num_concurrent} ä¸ªå¹¶å‘æŸ¥è¯¢...")
        start_time = time.time()
        tasks = [run_query() for _ in range(num_concurrent)]
        times = await asyncio.gather(*tasks)
        total_time = time.time() - start_time
        
        avg_time = statistics.mean(times)
        throughput = num_concurrent / total_time
        
        self.results['concurrent_load'] = {
            'num_concurrent': num_concurrent,
            'total_time': total_time,
            'avg_query_time': avg_time,
            'throughput': throughput
        }
        
        print(f"\n  âœ“ å¹¶å‘æµ‹è¯•å®Œæˆ:")
        print(f"    - å¹¶å‘æ•°: {num_concurrent}")
        print(f"    - æ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"    - å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_time:.3f}ç§’")
        print(f"    - ååé‡: {throughput:.2f} æŸ¥è¯¢/ç§’")
    
    async def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*60)
        print("æ€§èƒ½æµ‹è¯•æ€»ç»“")
        print("="*60)
        
        print("\nğŸ“„ æ–‡æ¡£å¤„ç†æ€§èƒ½:")
        doc_perf = self.results['document_processing']
        print(f"  â€¢ å¤„ç†é€Ÿåº¦: {doc_perf['docs_per_second']:.2f} æ–‡æ¡£/ç§’")
        print(f"  â€¢ å¹³å‡è€—æ—¶: {doc_perf['avg_time_per_doc']:.3f} ç§’/æ–‡æ¡£")
        
        print("\nğŸ” æœç´¢æ€§èƒ½:")
        for mode, metrics in self.results['search_performance'].items():
            print(f"  â€¢ {mode.upper()}æ¨¡å¼:")
            print(f"    - æŸ¥è¯¢é€Ÿåº¦: {metrics['queries_per_second']:.2f} æŸ¥è¯¢/ç§’")
            print(f"    - å¹³å‡è€—æ—¶: {metrics['avg_time']:.3f} ç§’")
            print(f"    - è€—æ—¶èŒƒå›´: {metrics['min_time']:.3f}~{metrics['max_time']:.3f} ç§’")
        
        print("\nğŸ’¾ ç¼“å­˜æ€§èƒ½:")
        cache_perf = self.results['cache_performance']
        print(f"  â€¢ æ— ç¼“å­˜: {cache_perf['cold_start_time']:.3f} ç§’")
        print(f"  â€¢ æœ‰ç¼“å­˜: {cache_perf['cached_time']:.3f} ç§’")
        print(f"  â€¢ åŠ é€Ÿæ¯”: {cache_perf['speedup']:.2f}x")
        
        print("\nâš¡ å¹¶å‘æ€§èƒ½:")
        concurrent = self.results['concurrent_load']
        print(f"  â€¢ å¹¶å‘æ•°: {concurrent['num_concurrent']}")
        print(f"  â€¢ ååé‡: {concurrent['throughput']:.2f} æŸ¥è¯¢/ç§’")
        
        # æ€§èƒ½è¯„çº§
        print("\nğŸ“Š æ€§èƒ½è¯„çº§:")
        doc_speed = doc_perf['docs_per_second']
        search_speed = self.results['search_performance']['hybrid']['queries_per_second']
        
        def rate_performance(value, thresholds):
            if value >= thresholds[0]:
                return "ä¼˜ç§€ â­â­â­â­â­"
            elif value >= thresholds[1]:
                return "è‰¯å¥½ â­â­â­â­"
            elif value >= thresholds[2]:
                return "ä¸€èˆ¬ â­â­â­"
            else:
                return "éœ€ä¼˜åŒ– â­â­"
        
        print(f"  â€¢ æ–‡æ¡£å¤„ç†: {rate_performance(doc_speed, [5, 2, 1])}")
        print(f"  â€¢ æœç´¢é€Ÿåº¦: {rate_performance(search_speed, [20, 10, 5])}")
        print(f"  â€¢ ç¼“å­˜æ•ˆæœ: {rate_performance(cache_perf['speedup'], [10, 5, 2])}")
        
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        print("="*60)
    
    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        print("\n[7] æ¸…ç†æµ‹è¯•æ•°æ®...")
        await self.kb.clear()
        print("âœ“ æµ‹è¯•æ•°æ®å·²æ¸…ç†")


async def run_benchmark():
    """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    benchmark = PerformanceBenchmark()
    
    try:
        # è®¾ç½®
        await benchmark.setup()
        
        # æ–‡æ¡£å¤„ç†æµ‹è¯•
        await benchmark.benchmark_document_processing(num_docs=50)
        
        # æœç´¢æ€§èƒ½æµ‹è¯•
        await benchmark.benchmark_search_performance(num_queries=20)
        
        # ç¼“å­˜æ€§èƒ½æµ‹è¯•
        await benchmark.benchmark_cache_performance(num_queries=50)
        
        # å¹¶å‘è´Ÿè½½æµ‹è¯•
        await benchmark.benchmark_concurrent_load(num_concurrent=10)
        
        # æ‰“å°æ€»ç»“
        await benchmark.print_summary()
        
    finally:
        # æ¸…ç†
        await benchmark.cleanup()


if __name__ == "__main__":
    asyncio.run(run_benchmark())
