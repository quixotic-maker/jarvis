# Jarvis LLM Providerç³»ç»Ÿ

## ğŸ¯ æ¦‚è¿°

Jarvisçš„LLM Providerç³»ç»Ÿæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„æŠ½è±¡å±‚ï¼Œæ”¯æŒå¤šä¸ªLLMæä¾›å•†ï¼ŒåŒ…æ‹¬OpenAIã€Anthropicå’ŒOllamaï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰ã€‚

## ğŸ—ï¸ æ¶æ„

```
app/core/
â”œâ”€â”€ llm_provider.py      # æŠ½è±¡åŸºç±»
â”œâ”€â”€ llm_config.py        # é…ç½®ç®¡ç†
â”œâ”€â”€ llm_factory.py       # Providerå·¥å‚
â””â”€â”€ providers/
    â”œâ”€â”€ openai_provider.py      # OpenAIå®ç°
    â”œâ”€â”€ anthropic_provider.py   # Anthropicå®ç°
    â””â”€â”€ ollama_provider.py      # Ollamaå®ç°
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. é…ç½®ç¯å¢ƒå˜é‡

å¤åˆ¶ `.env.example` åˆ° `.env` å¹¶å¡«å…¥ä½ çš„APIå¯†é’¥ï¼š

```bash
cp .env.example .env
```

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```env
# OpenAI
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_MODEL=gpt-3.5-turbo

# Anthropic
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Ollama (æœ¬åœ°)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# é»˜è®¤Provider
DEFAULT_PROVIDER=OPENAI
```

### 2. å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

ä¸»è¦ä¾èµ–ï¼š
- `openai>=1.10.0` - OpenAIå®˜æ–¹SDK
- `anthropic>=0.8.1` - Anthropicå®˜æ–¹SDK
- `tiktoken>=0.5.2` - Tokenè®¡æ•°åº“
- `httpx>=0.26.0` - HTTPå®¢æˆ·ç«¯ï¼ˆç”¨äºOllamaï¼‰

### 3. æµ‹è¯•Provider

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
cd backend
python test_llm_providers.py
```

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### Pythonä»£ç ä¸­ä½¿ç”¨

```python
from app.core.llm_factory import get_provider
from app.core.llm_config import LLMProviderType
from app.core.llm_provider import ChatRequest, Message

# 1. ä½¿ç”¨é»˜è®¤provider
provider = get_provider()

# 2. æŒ‡å®šproviderç±»å‹
provider = get_provider(LLMProviderType.OPENAI)

# 3. åˆ›å»ºèŠå¤©è¯·æ±‚
request = ChatRequest(
    messages=[
        Message(role="user", content="Hello, how are you?")
    ],
    temperature=0.7,
    max_tokens=100
)

# 4. éæµå¼å¯¹è¯
response = await provider.chat(request)
print(response.content)
print(response.usage)  # tokenä½¿ç”¨æƒ…å†µ

# 5. æµå¼å¯¹è¯
async for chunk in provider.chat_stream(request):
    print(chunk, end="", flush=True)

# 6. Tokenè®¡æ•°
token_count = await provider.count_tokens("Hello world", "gpt-3.5-turbo")
print(f"Tokens: {token_count}")

# 7. è·å–å¯ç”¨æ¨¡å‹
models = provider.get_available_models()
print(f"Available models: {models}")
```

### REST APIä½¿ç”¨

#### 1. èŠå¤©è¡¥å…¨

```bash
curl -X POST http://localhost:8000/api/v2/llm/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"}
    ],
    "provider": "openai",
    "model": "gpt-3.5-turbo",
    "temperature": 0.7,
    "max_tokens": 500,
    "stream": false
  }'
```

å“åº”ï¼š
```json
{
  "status": "success",
  "message": "èŠå¤©è¡¥å…¨æˆåŠŸ",
  "data": {
    "content": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯...",
    "model": "gpt-3.5-turbo",
    "usage": {
      "prompt_tokens": 12,
      "completion_tokens": 156,
      "total_tokens": 168
    },
    "finish_reason": "stop"
  }
}
```

#### 2. æµå¼å¯¹è¯

```bash
curl -X POST http://localhost:8000/api/v2/llm/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "å†™ä¸€é¦–è¯—"}
    ],
    "provider": "openai",
    "stream": true
  }'
```

å“åº”ï¼ˆSSEæµï¼‰ï¼š
```
data: {"content": "æ˜¥"}
data: {"content": "å¤©"}
data: {"content": "æ¥"}
...
data: [DONE]
```

#### 3. è·å–å¯ç”¨æ¨¡å‹

```bash
curl http://localhost:8000/api/v2/llm/models?provider=openai
```

å“åº”ï¼š
```json
{
  "status": "success",
  "message": "è·å–æ¨¡å‹åˆ—è¡¨æˆåŠŸ",
  "data": {
    "provider": "openai",
    "models": ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"]
  }
}
```

#### 4. Tokenè®¡æ•°

```bash
curl -X POST http://localhost:8000/api/v2/llm/count-tokens \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello, world!",
    "model": "gpt-3.5-turbo",
    "provider": "openai"
  }'
```

## ğŸ”§ Providerè¯¦è§£

### DeepSeek Provider â­ æ¨è

**æ”¯æŒçš„æ¨¡å‹**:
- deepseek-chat (é€šç”¨å¯¹è¯æ¨¡å‹)
- deepseek-coder (ä»£ç ä¸“ç”¨æ¨¡å‹)

**ç‰¹æ€§**:
- âœ… å…¼å®¹OpenAI APIæ ¼å¼
- âœ… **æé«˜æ€§ä»·æ¯”**ï¼ˆçº¦ä¸ºGPT-3.5çš„1/5æˆæœ¬ï¼‰
- âœ… 32Kä¸Šä¸‹æ–‡çª—å£
- âœ… ä¸­æ–‡èƒ½åŠ›å¼º
- âœ… ç²¾ç¡®çš„tokenè®¡æ•°ï¼ˆä½¿ç”¨tiktokenï¼‰
- âœ… æµå¼å¯¹è¯æ”¯æŒ

**å®šä»·**ï¼ˆæ¯1000 tokensï¼‰:
- deepseek-chat: $0.00014 (è¾“å…¥) / $0.00028 (è¾“å‡º)
- deepseek-coder: $0.00014 (è¾“å…¥) / $0.00028 (è¾“å‡º)
- **çº¦Â¥1/ç™¾ä¸‡tokens** - æ€§ä»·æ¯”ä¹‹ç‹ï¼

**é…ç½®æ–¹æ³•**:
```env
# åœ¨.envä¸­é…ç½®DeepSeek
DEEPSEEK_API_KEY=sk-your-deepseek-key
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MODEL=deepseek-chat

# ä½¿ç”¨OpenAI providerè°ƒç”¨DeepSeek
DEFAULT_PROVIDER=OPENAI
```

ç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹ï¼šå¦‚æœé…ç½®äº†`DEEPSEEK_API_KEY`ï¼ŒOpenAI providerä¼šä¼˜å…ˆä½¿ç”¨DeepSeeké…ç½®ã€‚

### OpenAI Provider

**æ”¯æŒçš„æ¨¡å‹**:
- GPT-4
- GPT-4 Turbo
- GPT-3.5 Turbo

**ç‰¹æ€§**:
- âœ… ç²¾ç¡®çš„tokenè®¡æ•°ï¼ˆä½¿ç”¨tiktokenï¼‰
- âœ… å‡†ç¡®çš„æˆæœ¬è®¡ç®—
- âœ… æµå¼å¯¹è¯æ”¯æŒ
- âœ… å®Œæ•´çš„APIæ”¯æŒ

**å®šä»·**ï¼ˆæ¯1000 tokensï¼‰:
- GPT-4: $0.03 (è¾“å…¥) / $0.06 (è¾“å‡º)
- GPT-4 Turbo: $0.01 (è¾“å…¥) / $0.03 (è¾“å‡º)
- GPT-3.5 Turbo: $0.0005 (è¾“å…¥) / $0.0015 (è¾“å‡º)

### Anthropic Provider

**æ”¯æŒçš„æ¨¡å‹**:
- Claude-3 Opus
- Claude-3 Sonnet
- Claude-3 Haiku

**ç‰¹æ€§**:
- âœ… Systemæ¶ˆæ¯è‡ªåŠ¨åˆ†ç¦»
- âœ… æµå¼å¯¹è¯æ”¯æŒ
- âœ… æˆæœ¬è·Ÿè¸ª
- âš ï¸ Tokenè®¡æ•°ä¸ºè¿‘ä¼¼å€¼ï¼ˆæ¯3å­—ç¬¦çº¦1 tokenï¼‰

**å®šä»·**ï¼ˆæ¯1000 tokensï¼‰:
- Claude-3 Opus: $0.015 (è¾“å…¥) / $0.075 (è¾“å‡º)
- Claude-3 Sonnet: $0.003 (è¾“å…¥) / $0.015 (è¾“å‡º)
- Claude-3 Haiku: $0.00025 (è¾“å…¥) / $0.00125 (è¾“å‡º)

### Ollama Provider

**æ”¯æŒçš„æ¨¡å‹**:
- Llama 2
- Mistral
- Mixtral
- Code Llama
- Phi
- Qwen

**ç‰¹æ€§**:
- âœ… æœ¬åœ°è¿è¡Œï¼Œå…è´¹
- âœ… æµå¼å¯¹è¯æ”¯æŒ
- âœ… å®Œå…¨ç§å¯†
- âš ï¸ Tokenè®¡æ•°ä¸ºè¿‘ä¼¼å€¼
- âš ï¸ éœ€è¦æœ¬åœ°è¿è¡ŒOllamaæœåŠ¡

**ä½¿ç”¨å‰æ**:
```bash
# å®‰è£…Ollama
curl https://ollama.ai/install.sh | sh

# å¯åŠ¨OllamaæœåŠ¡
ollama serve

# ä¸‹è½½æ¨¡å‹
ollama pull llama2
```

## ğŸ“Š æˆæœ¬è·Ÿè¸ª

ç³»ç»Ÿè‡ªåŠ¨è·Ÿè¸ªæ‰€æœ‰LLMè°ƒç”¨çš„æˆæœ¬ï¼š

```python
# æˆæœ¬è‡ªåŠ¨è®¡ç®—å¹¶è®°å½•
response = await provider.chat(request)

# è®¿é—®æˆæœ¬ä¿¡æ¯
usage_stats = UsageStats(
    provider=LLMProviderType.OPENAI,
    model="gpt-3.5-turbo",
    prompt_tokens=100,
    completion_tokens=50,
    total_tokens=150,
    cost=0.000225  # è‡ªåŠ¨è®¡ç®—
)
```

## ğŸ” å®‰å…¨æ€§

1. **APIå¯†é’¥ç®¡ç†**: æ‰€æœ‰å¯†é’¥å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­
2. **è¯·æ±‚éªŒè¯**: è‡ªåŠ¨éªŒè¯æ‰€æœ‰è¯·æ±‚å‚æ•°
3. **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶
4. **é€Ÿç‡é™åˆ¶**: å†…ç½®é€Ÿç‡é™åˆ¶æ”¯æŒï¼ˆå¯é…ç½®ï¼‰

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„Provider

- **OpenAI**: æœ€å¼ºå¤§ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡
- **Anthropic**: é•¿ä¸Šä¸‹æ–‡ï¼Œé€‚åˆæ–‡æ¡£åˆ†æ
- **Ollama**: å…è´¹æœ¬åœ°ï¼Œé€‚åˆå¼€å‘æµ‹è¯•

### 2. ä¼˜åŒ–æˆæœ¬

```python
# ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹å¤„ç†ç®€å•ä»»åŠ¡
provider = get_provider(LLMProviderType.OPENAI)
config_override = {"model": "gpt-3.5-turbo"}
cheap_provider = get_provider(
    LLMProviderType.OPENAI,
    config_override=config_override
)
```

### 3. æµå¼å“åº”

å¯¹äºç”¨æˆ·äº¤äº’ï¼Œä½¿ç”¨æµå¼å“åº”æä¾›æ›´å¥½çš„ä½“éªŒï¼š

```python
async for chunk in provider.chat_stream(request):
    # å®æ—¶æ˜¾ç¤ºç»™ç”¨æˆ·
    yield chunk
```

### 4. é”™è¯¯å¤„ç†

```python
try:
    response = await provider.chat(request)
except ValueError as e:
    # å‚æ•°éªŒè¯é”™è¯¯
    print(f"Invalid request: {e}")
except RuntimeError as e:
    # APIè°ƒç”¨å¤±è´¥
    print(f"API error: {e}")
```

## ğŸ§ª æµ‹è¯•

è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶ï¼š

```bash
python test_llm_providers.py
```

æµ‹è¯•è¦†ç›–ï¼š
- âœ… éæµå¼å¯¹è¯
- âœ… æµå¼å¯¹è¯
- âœ… Tokenè®¡æ•°
- âœ… æ¨¡å‹åˆ—è¡¨
- âœ… æˆæœ¬è®¡ç®—

## ğŸš€ æœªæ¥è®¡åˆ’

- [ ] æ·»åŠ æ›´å¤šProviderï¼ˆGoogle PaLMã€Cohereç­‰ï¼‰
- [ ] å®ç°è‡ªåŠ¨é‡è¯•å’Œfallbackæœºåˆ¶
- [ ] æ·»åŠ ç¼“å­˜å±‚å‡å°‘é‡å¤è°ƒç”¨
- [ ] æ•°æ®åº“æŒä¹…åŒ–ä½¿ç”¨ç»Ÿè®¡
- [ ] Web UIç®¡ç†ç•Œé¢
- [ ] æˆæœ¬é¢„ç®—å’Œå‘Šè­¦

## ğŸ“ APIæ–‡æ¡£

è®¿é—® http://localhost:8000/docs æŸ¥çœ‹å®Œæ•´çš„Swagger APIæ–‡æ¡£ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®æ–°çš„Providerå®ç°ï¼åªéœ€ç»§æ‰¿ `LLMProvider` åŸºç±»å¹¶å®ç°æ‰€æœ‰æŠ½è±¡æ–¹æ³•ã€‚

## ğŸ“„ è®¸å¯

MIT License
