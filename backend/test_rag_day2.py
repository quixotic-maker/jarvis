"""
RAGç³»ç»Ÿ Day 2-3 æµ‹è¯•å¥—ä»¶
æµ‹è¯•æ–‡æ¡£å¤„ç†Pipelineï¼šåŠ è½½ -> åˆ†å— -> è½¬æ¢
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from app.rag.chunking import ChunkingService, ChunkingStrategy
from app.rag.loaders import (
    TextLoader, MarkdownLoader, CodeLoader, PDFLoader,
    LoaderFactory, get_loader_factory
)
from app.rag.document_processor import DocumentProcessor, get_document_processor


def test_chunking_service():
    """æµ‹è¯•åˆ†å—æœåŠ¡"""
    print("\n" + "="*60)
    print("æµ‹è¯•1: ChunkingService - æ–‡æœ¬åˆ†å—åŠŸèƒ½")
    print("="*60)
    
    # æµ‹è¯•å›ºå®šå¤§å°åˆ†å—
    print("\n[æµ‹è¯•1.1] å›ºå®šå¤§å°åˆ†å—")
    chunking_service = ChunkingService(
        chunk_size=100,
        chunk_overlap=20,
        strategy=ChunkingStrategy.FIXED_SIZE
    )
    
    test_text = """è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡å­—ã€‚åŒ…å«äº†ä¸€äº›å†…å®¹ã€‚
è¿™æ˜¯ç¬¬äºŒæ®µæ–‡å­—ã€‚å®ƒæ›´é•¿ä¸€äº›ï¼ŒåŒ…å«æ›´å¤šçš„ä¿¡æ¯å’Œç»†èŠ‚ã€‚
è¿™æ˜¯ç¬¬ä¸‰æ®µæ–‡å­—ã€‚ç”¨äºæµ‹è¯•åˆ†å—åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚
æœ€åä¸€æ®µæ–‡å­—ï¼Œç”¨äºéªŒè¯é‡å åŠŸèƒ½ã€‚"""
    
    chunks = chunking_service.chunk_text(test_text)
    print(f"âœ“ æ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")
    print(f"âœ“ åˆ†å—æ•°é‡: {len(chunks)} ä¸ª")
    
    for i, chunk in enumerate(chunks):
        print(f"\n  å— {i+1}:")
        print(f"    æ–‡æœ¬: {chunk.text[:50]}...")
        print(f"    å¤§å°: {len(chunk.text)} å­—ç¬¦")
        print(f"    ä½ç½®: {chunk.start_index}-{chunk.end_index}")
        print(f"    å…ƒæ•°æ®: {chunk.metadata}")
    
    # æµ‹è¯•å¥å­åˆ†å—
    print("\n[æµ‹è¯•1.2] å¥å­åˆ†å—")
    sentence_chunking = ChunkingService(
        chunk_size=150,
        chunk_overlap=30,
        strategy=ChunkingStrategy.SENTENCE
    )
    
    sentence_text = """Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ã€‚å®ƒè¢«å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸã€‚
æ•°æ®ç§‘å­¦æ˜¯Pythonçš„é‡è¦åº”ç”¨æ–¹å‘ã€‚è®¸å¤šæ•°æ®ç§‘å­¦å®¶é€‰æ‹©ä½¿ç”¨Pythonã€‚
æœºå™¨å­¦ä¹ ä¹Ÿæ˜¯Pythonçš„å¼ºé¡¹ã€‚TensorFlowå’ŒPyTorchéƒ½æ”¯æŒPythonã€‚"""
    
    sentence_chunks = sentence_chunking.chunk_text(sentence_text)
    print(f"âœ“ å¥å­åˆ†å—æ•°é‡: {len(sentence_chunks)} ä¸ª")
    
    for i, chunk in enumerate(sentence_chunks):
        print(f"  å— {i+1}: {chunk.metadata.get('sentence_count')} å¥")
    
    # æµ‹è¯•ä»£ç åˆ†å—
    print("\n[æµ‹è¯•1.3] ä»£ç åˆ†å—")
    code_chunking = ChunkingService(
        chunk_size=200,
        chunk_overlap=50,
        strategy=ChunkingStrategy.CODE
    )
    
    code_text = """def hello_world():
    print("Hello, World!")

def add(a, b):
    return a + b

class Calculator:
    def __init__(self):
        self.result = 0
    
    def add(self, x):
        self.result += x
        return self.result
"""
    
    code_chunks = code_chunking.chunk_text(code_text)
    print(f"âœ“ ä»£ç åˆ†å—æ•°é‡: {len(code_chunks)} ä¸ª")
    
    print("\nâœ… ChunkingServiceæµ‹è¯•é€šè¿‡")


def test_loaders():
    """æµ‹è¯•æ–‡æ¡£åŠ è½½å™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯•2: DocumentLoaders - å¤šæ ¼å¼æ–‡æ¡£åŠ è½½")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶ç›®å½•
    test_dir = Path(__file__).parent / "test_data"
    test_dir.mkdir(exist_ok=True)
    
    # æµ‹è¯•æ–‡æœ¬åŠ è½½å™¨
    print("\n[æµ‹è¯•2.1] TextLoader - çº¯æ–‡æœ¬åŠ è½½")
    text_file = test_dir / "test.txt"
    text_file.write_text("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬æ–‡ä»¶ã€‚\nåŒ…å«å¤šè¡Œå†…å®¹ã€‚\nç”¨äºæµ‹è¯•æ–‡æœ¬åŠ è½½å™¨ã€‚")
    
    text_loader = TextLoader()
    result = text_loader.load(str(text_file))
    print(f"âœ“ åŠ è½½æˆåŠŸ")
    print(f"  å†…å®¹é•¿åº¦: {len(result['content'])} å­—ç¬¦")
    print(f"  è¡Œæ•°: {result['metadata']['line_count']}")
    print(f"  ç¼–ç : {result['metadata']['encoding']}")
    print(f"  åŠ è½½å™¨: {result['metadata']['loader_type']}")
    
    # æµ‹è¯•MarkdownåŠ è½½å™¨
    print("\n[æµ‹è¯•2.2] MarkdownLoader - Markdownæ–‡æ¡£åŠ è½½")
    md_file = test_dir / "test.md"
    md_content = """# æ ‡é¢˜1

è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ã€‚

## æ ‡é¢˜2

è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ï¼ŒåŒ…å«[é“¾æ¥](https://example.com)ã€‚

```python
def hello():
    print("Hello")
```

![å›¾ç‰‡](image.png)
"""
    md_file.write_text(md_content)
    
    md_loader = MarkdownLoader()
    md_result = md_loader.load(str(md_file))
    print(f"âœ“ åŠ è½½æˆåŠŸ")
    print(f"  å†…å®¹é•¿åº¦: {len(md_result['content'])} å­—ç¬¦")
    print(f"  æ ‡é¢˜: {md_result['metadata'].get('title', 'N/A')}")
    print(f"  æ ‡é¢˜æ•°é‡: {md_result['metadata'].get('header_count', 0)}")
    print(f"  ä»£ç å—æ•°é‡: {md_result['metadata'].get('code_block_count', 0)}")
    print(f"  é“¾æ¥æ•°é‡: {md_result['metadata'].get('link_count', 0)}")
    print(f"  å›¾ç‰‡æ•°é‡: {md_result['metadata'].get('image_count', 0)}")
    
    # æµ‹è¯•ä»£ç åŠ è½½å™¨
    print("\n[æµ‹è¯•2.3] CodeLoader - ä»£ç æ–‡ä»¶åŠ è½½")
    py_file = test_dir / "test.py"
    py_content = """# Pythonæµ‹è¯•æ–‡ä»¶
import os
import sys

class TestClass:
    '''æµ‹è¯•ç±»'''
    def __init__(self):
        self.value = 0
    
    def method1(self):
        return self.value

def function1(x, y):
    '''æµ‹è¯•å‡½æ•°'''
    return x + y

def function2():
    pass
"""
    py_file.write_text(py_content)
    
    code_loader = CodeLoader()
    code_result = code_loader.load(str(py_file))
    print(f"âœ“ åŠ è½½æˆåŠŸ")
    print(f"  è¯­è¨€: {code_result['metadata']['language']}")
    print(f"  è¡Œæ•°: {code_result['metadata']['line_count']}")
    print(f"  ç±»æ•°é‡: {code_result['metadata'].get('class_count', 0)}")
    print(f"  å‡½æ•°æ•°é‡: {code_result['metadata'].get('function_count', 0)}")
    print(f"  å¯¼å…¥æ•°é‡: {code_result['metadata'].get('import_count', 0)}")
    print(f"  ä»£ç å¯†åº¦: {code_result['metadata'].get('code_density', 0)}")
    
    # æµ‹è¯•åŠ è½½å™¨å·¥å‚
    print("\n[æµ‹è¯•2.4] LoaderFactory - è‡ªåŠ¨é€‰æ‹©åŠ è½½å™¨")
    factory = get_loader_factory()
    
    # æµ‹è¯•è‡ªåŠ¨åŠ è½½å„ç§æ ¼å¼
    for test_file in [text_file, md_file, py_file]:
        result = factory.load_document(str(test_file))
        if result:
            loader_type = result['metadata']['loader_type']
            print(f"âœ“ {test_file.name}: {loader_type}")
        else:
            print(f"âœ— {test_file.name}: åŠ è½½å¤±è´¥")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    for f in test_dir.glob('*'):
        if f.is_file():
            f.unlink()
    
    print("\nâœ… Loadersæµ‹è¯•é€šè¿‡")


def test_document_processor():
    """æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯•3: DocumentProcessor - æ–‡æ¡£å¤„ç†Pipeline")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_dir = Path(__file__).parent / "test_data"
    test_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯•Markdownæ–‡ä»¶
    md_file = test_dir / "test_doc.md"
    md_content = """# RAGç³»ç»Ÿä»‹ç»

RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯ã€‚

## æ ¸å¿ƒç»„ä»¶

1. **å‘é‡æ•°æ®åº“**: å­˜å‚¨æ–‡æ¡£çš„å‘é‡è¡¨ç¤º
2. **æ£€ç´¢æœåŠ¡**: æ ¹æ®æŸ¥è¯¢æ£€ç´¢ç›¸å…³æ–‡æ¡£
3. **ç”ŸæˆæœåŠ¡**: åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”

## æŠ€æœ¯ä¼˜åŠ¿

- æé«˜ç­”æ¡ˆå‡†ç¡®æ€§
- é™ä½å¹»è§‰é—®é¢˜
- æ”¯æŒçŸ¥è¯†åº“æ›´æ–°

## åº”ç”¨åœºæ™¯

RAGå¯ä»¥åº”ç”¨äºé—®ç­”ç³»ç»Ÿã€æ™ºèƒ½å®¢æœã€æ–‡æ¡£åŠ©æ‰‹ç­‰å¤šä¸ªé¢†åŸŸã€‚
é€šè¿‡ç»“åˆå¤–éƒ¨çŸ¥è¯†åº“ï¼Œå¤§è¯­è¨€æ¨¡å‹èƒ½å¤Ÿæä¾›æ›´å‡†ç¡®ã€æ›´åŠæ—¶çš„ä¿¡æ¯ã€‚
""" * 3  # é‡å¤3æ¬¡ä»¥è·å¾—æ›´é•¿çš„æ–‡æœ¬
    
    md_file.write_text(md_content)
    
    # æµ‹è¯•å•æ–‡ä»¶å¤„ç†
    print("\n[æµ‹è¯•3.1] å¤„ç†å•ä¸ªæ–‡ä»¶")
    processor = get_document_processor(
        chunk_size=500,
        chunk_overlap=100
    )
    
    documents = processor.process_file(
        str(md_file),
        additional_metadata={"source": "test", "category": "documentation"}
    )
    
    print(f"âœ“ æ–‡ä»¶å¤„ç†æˆåŠŸ")
    print(f"  åŸæ–‡ä»¶: {md_file.name}")
    print(f"  ç”Ÿæˆæ–‡æ¡£æ•°: {len(documents)} ä¸ª")
    print(f"\n  æ–‡æ¡£è¯¦æƒ…:")
    
    for i, doc in enumerate(documents[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"\n  æ–‡æ¡£ {i+1}:")
        print(f"    ID: {doc.id}")
        print(f"    å†…å®¹: {doc.content[:100]}...")
        print(f"    é•¿åº¦: {len(doc.content)} å­—ç¬¦")
        print(f"    å…ƒæ•°æ®é”®: {list(doc.metadata.keys())}")
    
    if len(documents) > 3:
        print(f"\n  ... è¿˜æœ‰ {len(documents) - 3} ä¸ªæ–‡æ¡£")
    
    # æµ‹è¯•æ–‡æœ¬ç›´æ¥å¤„ç†
    print("\n[æµ‹è¯•3.2] å¤„ç†çº¯æ–‡æœ¬")
    text = """è¿™æ˜¯ä¸€æ®µé•¿æ–‡æœ¬ã€‚""" * 100
    text_docs = processor.process_text(
        text,
        metadata={"type": "plain_text"}
    )
    print(f"âœ“ æ–‡æœ¬å¤„ç†æˆåŠŸ: {len(text_docs)} ä¸ªæ–‡æ¡£")
    
    # æµ‹è¯•ç›®å½•å¤„ç†
    print("\n[æµ‹è¯•3.3] å¤„ç†ç›®å½•")
    # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ–‡ä»¶
    (test_dir / "doc1.txt").write_text("æ–‡æ¡£1çš„å†…å®¹" * 50)
    (test_dir / "doc2.md").write_text("# æ–‡æ¡£2\nå†…å®¹" * 50)
    (test_dir / "code.py").write_text("def test():\n    pass\n" * 20)
    
    dir_docs = processor.process_directory(
        str(test_dir),
        recursive=False,
        file_patterns=['*.txt', '*.md', '*.py']
    )
    
    print(f"âœ“ ç›®å½•å¤„ç†æˆåŠŸ")
    print(f"  æ€»æ–‡æ¡£æ•°: {len(dir_docs)} ä¸ª")
    
    # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
    file_types = {}
    for doc in dir_docs:
        ext = doc.metadata.get('file_extension', 'unknown')
        file_types[ext] = file_types.get(ext, 0) + 1
    
    print(f"  æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
    for ext, count in file_types.items():
        print(f"    {ext}: {count} ä¸ªæ–‡æ¡£")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    for f in test_dir.glob('*'):
        if f.is_file():
            f.unlink()
    test_dir.rmdir()
    
    print("\nâœ… DocumentProcessoræµ‹è¯•é€šè¿‡")


def test_integration():
    """é›†æˆæµ‹è¯•ï¼šå®Œæ•´æµç¨‹"""
    print("\n" + "="*60)
    print("æµ‹è¯•4: é›†æˆæµ‹è¯• - å®Œæ•´æ–‡æ¡£å¤„ç†æµç¨‹")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_dir = Path(__file__).parent / "test_data"
    test_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_doc = test_dir / "integration_test.md"
    content = """# Pythonç¼–ç¨‹æœ€ä½³å®è·µ

## 1. ä»£ç é£æ ¼

éµå¾ªPEP 8è§„èŒƒï¼Œä½¿ç”¨ä¸€è‡´çš„å‘½åå’Œæ ¼å¼ã€‚

## 2. æ–‡æ¡£å­—ç¬¦ä¸²

ä¸ºæ‰€æœ‰å…¬å…±å‡½æ•°å’Œç±»ç¼–å†™æ¸…æ™°çš„æ–‡æ¡£å­—ç¬¦ä¸²ã€‚

## 3. é”™è¯¯å¤„ç†

ä½¿ç”¨try-exceptæ•è·å’Œå¤„ç†å¼‚å¸¸ã€‚

## 4. æµ‹è¯•

ç¼–å†™å•å…ƒæµ‹è¯•ç¡®ä¿ä»£ç è´¨é‡ã€‚
"""
    test_doc.write_text(content)
    
    print("\n[æ­¥éª¤1] åˆå§‹åŒ–DocumentProcessor")
    processor = DocumentProcessor(
        chunk_size=200,
        chunk_overlap=50
    )
    print("âœ“ å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    print("\n[æ­¥éª¤2] åŠ è½½å¹¶å¤„ç†æ–‡æ¡£")
    documents = processor.process_file(str(test_doc))
    print(f"âœ“ ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£å—")
    
    print("\n[æ­¥éª¤3] éªŒè¯æ–‡æ¡£ç»“æ„")
    for i, doc in enumerate(documents):
        # éªŒè¯å¿…éœ€å­—æ®µ
        assert doc.id is not None, "æ–‡æ¡£IDä¸èƒ½ä¸ºç©º"
        assert doc.content is not None, "æ–‡æ¡£å†…å®¹ä¸èƒ½ä¸ºç©º"
        assert doc.metadata is not None, "æ–‡æ¡£å…ƒæ•°æ®ä¸èƒ½ä¸ºç©º"
        assert len(doc.content) > 0, "æ–‡æ¡£å†…å®¹ä¸èƒ½ä¸ºç©ºå­—ç¬¦ä¸²"
        
        # éªŒè¯å…ƒæ•°æ®å®Œæ•´æ€§
        required_keys = ['file_name', 'chunk_index', 'chunk_size']
        for key in required_keys:
            assert key in doc.metadata, f"ç¼ºå°‘å…ƒæ•°æ®é”®: {key}"
        
        print(f"  âœ“ æ–‡æ¡£ {i+1} éªŒè¯é€šè¿‡")
    
    print("\n[æ­¥éª¤4] éªŒè¯åˆ†å—è´¨é‡")
    # æ£€æŸ¥é‡å 
    if len(documents) > 1:
        chunk1_end = documents[0].content[-50:]
        chunk2_start = documents[1].content[:50:]
        
        # ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼å†…å®¹ï¼ˆé‡å éªŒè¯ï¼‰
        print(f"  ç¬¬1å—ç»“å°¾: ...{chunk1_end[-30:]}")
        print(f"  ç¬¬2å—å¼€å¤´: {chunk2_start[:30]}...")
        print("  âœ“ åˆ†å—é‡å æ£€æŸ¥å®Œæˆ")
    
    # æ¸…ç†
    test_doc.unlink()
    test_dir.rmdir()
    
    print("\nâœ… é›†æˆæµ‹è¯•é€šè¿‡")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("RAGç³»ç»Ÿ Day 2-3 æµ‹è¯•å¥—ä»¶")
    print("æµ‹è¯•èŒƒå›´: æ–‡æ¡£åŠ è½½ + åˆ†å— + å¤„ç†Pipeline")
    print("="*60)
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_chunking_service()
        test_loaders()
        test_document_processor()
        test_integration()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("="*60)
        print("\nâœ… Day 2-3 äº¤ä»˜ç‰©éªŒè¯å®Œæˆ:")
        print("  1. ChunkingService - 4ç§åˆ†å—ç­–ç•¥ âœ“")
        print("  2. DocumentLoaders - 4ç§æ–‡ä»¶æ ¼å¼ âœ“")
        print("  3. DocumentProcessor - å®Œæ•´Pipeline âœ“")
        print("  4. é›†æˆæµ‹è¯• - ç«¯åˆ°ç«¯æµç¨‹ âœ“")
        
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        return 1
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
