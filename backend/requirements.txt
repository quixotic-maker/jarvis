# Web Framework
fastapi==0.109.0































































































































































































































        raise HTTPException(status_code=500, detail=f"服务器错误: {str(e)}")    except Exception as e:                )            }                "model": model or "default"                "token_count": token_count,                "text_length": len(text),            data={            message="Token计数成功",            status=ResponseStatus.SUCCESS,        return BaseResponse(                token_count = await llm_provider.count_tokens(text, model)        llm_provider = get_provider(provider_type)        # 获取provider实例                    provider_type = provider_map.get(provider.lower())            }                "ollama": LLMProviderType.OLLAMA,                "anthropic": LLMProviderType.ANTHROPIC,                "openai": LLMProviderType.OPENAI,            provider_map = {        if provider:        provider_type = None        # 确定provider类型    try:    """    计算文本的token数量    """):    provider: Optional[str] = None    model: Optional[str] = None,    text: str,async def count_tokens(@router.post("/count-tokens")    )        }            ]                }                    "supported_models": ["llama2", "mistral", "mixtral", "codellama"]                    "description": "本地开源模型",                    "display_name": "Ollama",                    "name": "ollama",                {                },                    "supported_models": ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"]                    "description": "Anthropic Claude系列模型",                    "display_name": "Anthropic",                    "name": "anthropic",                {                },                    "supported_models": ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo"]                    "description": "OpenAI GPT系列模型",                    "display_name": "OpenAI",                    "name": "openai",                {            "providers": [        data={        message="获取provider列表成功",        status=ResponseStatus.SUCCESS,    return BaseResponse(    """    获取可用的LLM providers    """async def list_providers():@router.get("/providers")        raise HTTPException(status_code=500, detail=f"服务器错误: {str(e)}")    except Exception as e:                )            }                "models": models                "provider": provider or "default",            data={            message="获取模型列表成功",            status=ResponseStatus.SUCCESS,        return BaseResponse(                models = llm_provider.get_available_models()        llm_provider = get_provider(provider_type)        # 获取provider实例                    provider_type = provider_map.get(provider.lower())            }                "ollama": LLMProviderType.OLLAMA,                "anthropic": LLMProviderType.ANTHROPIC,                "openai": LLMProviderType.OPENAI,            provider_map = {        if provider:        provider_type = None        # 确定provider类型    try:    """    获取可用模型列表    """async def list_models(provider: Optional[str] = None):@router.get("/models")        raise HTTPException(status_code=500, detail=f"服务器错误: {str(e)}")    except Exception as e:        raise HTTPException(status_code=500, detail=str(e))    except RuntimeError as e:        raise HTTPException(status_code=400, detail=str(e))    except ValueError as e:                )            )                finish_reason=response.finish_reason                usage=response.usage,                model=response.model,                content=response.content,            data=ChatCompletionResponse(            message="聊天补全成功",            status=ResponseStatus.SUCCESS,        return BaseResponse(                response = await provider.chat(chat_request)        # 非流式响应                    )                media_type="text/event-stream"                generate(),            return StreamingResponse(                                yield f"data: {json.dumps(error_data)}\n\n"                    error_data = {"error": str(e)}                except Exception as e:                    yield "data: [DONE]\n\n"                        yield f"data: {json.dumps(data, ensure_ascii=False)}\n\n"                        data = {"content": chunk}                        # SSE格式                    async for chunk in provider.chat_stream(chat_request):                try:            async def generate():        if request.stream:        # 流式响应                )            stream=request.stream            max_tokens=request.max_tokens,            temperature=request.temperature,            model=request.model,            messages=messages,        chat_request = ChatRequest(        # 创建ChatRequest                ]            for msg in request.messages            Message(role=msg.role, content=msg.content)        messages = [        # 转换消息格式                provider = get_provider(provider_type)        # 获取provider实例                        )                    detail=f"不支持的provider: {request.provider}"                    status_code=400,                raise HTTPException(            if not provider_type:            provider_type = provider_map.get(request.provider.lower())            }                "ollama": LLMProviderType.OLLAMA,                "anthropic": LLMProviderType.ANTHROPIC,                "openai": LLMProviderType.OPENAI,            provider_map = {        if request.provider:        provider_type = None        # 确定provider类型    try:    """    支持多个LLM provider和流式/非流式响应    创建聊天补全    """async def create_chat_completion(request: ChatCompletionRequest):@router.post("/chat/completions")    finish_reason: str    usage: Dict[str, int]    model: str    content: str    """聊天补全响应"""class ChatCompletionResponse(BaseModel):    stream: bool = False    max_tokens: int = 2000    temperature: float = 0.7    provider: Optional[str] = None  # openai/anthropic/ollama    model: Optional[str] = None    messages: List[ChatMessage]    """聊天补全请求"""class ChatCompletionRequest(BaseModel):    content: str    role: str    """聊天消息"""class ChatMessage(BaseModel):router = APIRouter()import jsonfrom app.api.schemas import BaseResponse, ResponseStatusfrom app.core.llm_provider import ChatRequest, Messagefrom app.core.llm_config import LLMProviderTypefrom app.core.llm_factory import get_providerfrom typing import List, Optional, Dict, Anyfrom pydantic import BaseModelfrom fastapi.responses import StreamingResponsefrom fastapi import APIRouter, HTTPException"""提供统一的LLM调用接口uvicorn[standard]==0.27.0
python-multipart==0.0.6

# Database
sqlalchemy==2.0.25
alembic==1.13.1

# LLM Integration
openai==1.10.0
anthropic==0.8.1
tiktoken==0.5.2
langchain==0.1.5
langchain-openai==0.0.5

# Utilities
pydantic==2.5.3
pydantic-settings==2.1.0
python-dotenv==1.0.0
aiohttp==3.9.1
httpx==0.26.0

# Task Queue & Scheduling
celery==5.3.4
redis==5.0.1
apscheduler==3.10.4

# Data Processing
pandas==2.1.4
numpy==1.26.3

# Testing
pytest==7.4.4
pytest-asyncio==0.23.3
