#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•DeepSeek API
"""
import asyncio
from app.core.llm_factory import get_provider
from app.core.llm_config import LLMProviderType
from app.core.llm_provider import ChatRequest, Message


async def test_deepseek():
    """æµ‹è¯•DeepSeekå¯¹è¯"""
    print("ğŸš€ æµ‹è¯•DeepSeek API...")
    print("=" * 50)
    
    try:
        # è·å–Providerï¼ˆä¼šè‡ªåŠ¨ä½¿ç”¨DeepSeeké…ç½®ï¼‰
        provider = get_provider(LLMProviderType.OPENAI)
        
        print(f"âœ… Provideré…ç½®:")
        print(f"   API Key: {provider.api_key[:20]}...")
        print(f"   Base URL: {provider.base_url}")
        print(f"   Model: {provider.default_model}")
        print()
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        print("ğŸ“ æµ‹è¯•1: ç®€å•å¯¹è¯")
        request = ChatRequest(
            messages=[
                Message(role="user", content="ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½")
            ],
            temperature=0.7,
            max_tokens=100
        )
        
        response = await provider.chat(request)
        print(f"âœ… å“åº”: {response.content}")
        print(f"ğŸ“Š Tokenä½¿ç”¨: {response.usage}")
        print(f"ğŸ’° é¢„ä¼°æˆæœ¬: ~${(response.usage['total_tokens'] / 1000) * 0.00021:.6f} USD")
        print()
        
        # æµ‹è¯•æµå¼å¯¹è¯
        print("ğŸ“ æµ‹è¯•2: æµå¼å¯¹è¯")
        print("âœ… å“åº”: ", end="", flush=True)
        
        request2 = ChatRequest(
            messages=[
                Message(role="user", content="æ•°åˆ°5")
            ],
            temperature=0.7,
            max_tokens=50
        )
        
        async for chunk in provider.chat_stream(request2):
            print(chunk, end="", flush=True)
        print("\n")
        
        # æµ‹è¯•ä¸­æ–‡èƒ½åŠ›
        print("ğŸ“ æµ‹è¯•3: ä¸­æ–‡èƒ½åŠ›")
        request3 = ChatRequest(
            messages=[
                Message(role="user", content="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„äº”è¨€ç»å¥")
            ],
            temperature=0.8,
            max_tokens=100
        )
        
        response3 = await provider.chat(request3)
        print(f"âœ… å“åº”:\n{response3.content}")
        print(f"ğŸ“Š Tokenä½¿ç”¨: {response3.usage}")
        print()
        
        # æµ‹è¯•ä»£ç ç”Ÿæˆ
        print("ğŸ“ æµ‹è¯•4: ä»£ç ç”Ÿæˆèƒ½åŠ›")
        request4 = ChatRequest(
            messages=[
                Message(role="user", content="ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åºå‡½æ•°")
            ],
            temperature=0.3,
            max_tokens=200
        )
        
        response4 = await provider.chat(request4)
        print(f"âœ… å“åº”:\n{response4.content}")
        print(f"ğŸ“Š Tokenä½¿ç”¨: {response4.usage}")
        print()
        
        print("=" * 50)
        print("ğŸ‰ DeepSeek APIæµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        print()
        print("ğŸ’¡ æ€§ä»·æ¯”åˆ†æ:")
        total_tokens = (
            response.usage['total_tokens'] +
            response3.usage['total_tokens'] +
            response4.usage['total_tokens']
        )
        deepseek_cost = (total_tokens / 1000) * 0.00021
        gpt35_cost = (total_tokens / 1000) * 0.001
        print(f"   æ€»Tokenæ•°: {total_tokens}")
        print(f"   DeepSeekæˆæœ¬: ${deepseek_cost:.6f} USD")
        print(f"   GPT-3.5æˆæœ¬: ${gpt35_cost:.6f} USD")
        print(f"   èŠ‚çœ: {((gpt35_cost - deepseek_cost) / gpt35_cost * 100):.1f}%")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(test_deepseek())
