"""
RAGç³»ç»Ÿ Day 6-7 æµ‹è¯•å¥—ä»¶
æµ‹è¯•çŸ¥è¯†åº“ç®¡ç†æœåŠ¡ï¼šæ–‡æ¡£CRUDã€æ‰¹é‡å¯¼å…¥ã€å¯¼å‡ºã€ç»Ÿè®¡
"""
import asyncio
import sys
from pathlib import Path
import json
import tempfile
import shutil

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from app.rag.knowledge_base_service import (
    KnowledgeBaseService, get_knowledge_base, list_knowledge_bases
)
from app.rag.retrieval_service import RetrievalMode
from app.rag.embedding_service import EmbeddingProvider, get_embedding_service


async def test_kb_creation():
    """æµ‹è¯•çŸ¥è¯†åº“åˆ›å»ºå’Œåˆå§‹åŒ–"""
    print("\n" + "="*60)
    print("æµ‹è¯•1: çŸ¥è¯†åº“åˆ›å»ºå’Œåˆå§‹åŒ–")
    print("="*60)
    
    # åˆ›å»ºçŸ¥è¯†åº“
    kb = get_knowledge_base(
        kb_name="test_kb",
        description="æµ‹è¯•çŸ¥è¯†åº“",
        chunk_size=500,
        chunk_overlap=100
    )
    
    print(f"âœ“ çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ")
    print(f"  åç§°: {kb.kb_name}")
    print(f"  é›†åˆ: {kb.collection_name}")
    print(f"  åˆ†å—å¤§å°: {kb.metadata['chunk_size']}")
    
    # è·å–ç»Ÿè®¡
    stats = await kb.get_stats()
    print(f"âœ“ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ–‡æ¡£æ•°: {stats['document_count']}")
    
    # æ¸…ç©ºï¼ˆå‡†å¤‡æµ‹è¯•ï¼‰
    await kb.clear()
    print("âœ“ çŸ¥è¯†åº“å·²æ¸…ç©º")
    
    print("\nâœ… çŸ¥è¯†åº“åˆ›å»ºæµ‹è¯•é€šè¿‡")


async def test_add_text():
    """æµ‹è¯•æ·»åŠ æ–‡æœ¬"""
    print("\n" + "="*60)
    print("æµ‹è¯•2: æ·»åŠ æ–‡æœ¬åˆ°çŸ¥è¯†åº“")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    
    # æ·»åŠ å¤šä¸ªæ–‡æœ¬
    texts = [
        {
            "content": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåˆ›å»ºäº1991å¹´ã€‚Pythonè®¾è®¡å“²å­¦å¼ºè°ƒä»£ç å¯è¯»æ€§ï¼Œå…¶è¯­æ³•å…è®¸ç¨‹åºå‘˜ç”¨æ›´å°‘çš„ä»£ç è¡¨è¾¾æƒ³æ³•ã€‚",
            "metadata": {"category": "programming", "topic": "python", "language": "zh"}
        },
        {
            "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡è®­ç»ƒæ•°æ®æ¥è¯†åˆ«æ¨¡å¼ã€‚",
            "metadata": {"category": "ai", "topic": "machine_learning", "language": "zh"}
        },
        {
            "content": "Dockeræ˜¯ä¸€ä¸ªå¼€æºçš„å®¹å™¨åŒ–å¹³å°ï¼Œå®ƒå…è®¸å¼€å‘è€…å°†åº”ç”¨ç¨‹åºåŠå…¶ä¾èµ–é¡¹æ‰“åŒ…åˆ°ä¸€ä¸ªå¯ç§»æ¤çš„å®¹å™¨ä¸­ã€‚Dockerå®¹å™¨å¯ä»¥åœ¨ä»»ä½•æ”¯æŒDockerçš„ç³»ç»Ÿä¸Šè¿è¡Œã€‚",
            "metadata": {"category": "devops", "topic": "docker", "language": "zh"}
        }
    ]
    
    total_docs = 0
    for i, text_data in enumerate(texts):
        doc_ids = await kb.add_text(
            text=text_data["content"],
            metadata=text_data["metadata"]
        )
        total_docs += len(doc_ids)
        print(f"âœ“ æ–‡æœ¬ {i+1} æ·»åŠ æˆåŠŸ: {len(doc_ids)} ä¸ªæ–‡æ¡£å—")
    
    # éªŒè¯
    stats = await kb.get_stats()
    print(f"\nâœ“ æ€»è®¡æ·»åŠ : {total_docs} ä¸ªæ–‡æ¡£å—")
    print(f"âœ“ çŸ¥è¯†åº“æ–‡æ¡£æ•°: {stats['document_count']}")
    
    assert stats['document_count'] == total_docs, "æ–‡æ¡£æ•°ä¸åŒ¹é…"
    
    print("\nâœ… æ·»åŠ æ–‡æœ¬æµ‹è¯•é€šè¿‡")


async def test_add_file():
    """æµ‹è¯•æ·»åŠ æ–‡ä»¶"""
    print("\n" + "="*60)
    print("æµ‹è¯•3: æ·»åŠ æ–‡ä»¶åˆ°çŸ¥è¯†åº“")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
    temp_dir = Path(tempfile.mkdtemp())
    
    test_file = temp_dir / "test_doc.md"
    test_file.write_text("""# RAGç³»ç»Ÿæ–‡æ¡£

## ç®€ä»‹
RAG (Retrieval-Augmented Generation) æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯ã€‚

## æ ¸å¿ƒç»„ä»¶
1. å‘é‡æ•°æ®åº“
2. æ–‡æ¡£å¤„ç†å™¨
3. æ£€ç´¢æœåŠ¡
4. çŸ¥è¯†åº“ç®¡ç†

## åº”ç”¨åœºæ™¯
RAGå¯ç”¨äºé—®ç­”ç³»ç»Ÿã€æ™ºèƒ½å®¢æœã€æ–‡æ¡£åŠ©æ‰‹ç­‰é¢†åŸŸã€‚
""")
    
    # æ·»åŠ æ–‡ä»¶
    doc_ids = await kb.add_document(
        str(test_file),
        metadata={"source": "test", "type": "documentation"}
    )
    
    print(f"âœ“ æ–‡ä»¶æ·»åŠ æˆåŠŸ: {len(doc_ids)} ä¸ªæ–‡æ¡£å—")
    print(f"  æ–‡ä»¶: {test_file.name}")
    
    # æ¸…ç†
    shutil.rmtree(temp_dir)
    
    print("\nâœ… æ·»åŠ æ–‡ä»¶æµ‹è¯•é€šè¿‡")


async def test_search():
    """æµ‹è¯•çŸ¥è¯†åº“æœç´¢"""
    print("\n" + "="*60)
    print("æµ‹è¯•4: çŸ¥è¯†åº“æœç´¢")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    
    test_queries = [
        ("Pythonç¼–ç¨‹è¯­è¨€", "programming"),
        ("æœºå™¨å­¦ä¹ ç®—æ³•", "ai"),
        ("å®¹å™¨æŠ€æœ¯", "devops")
    ]
    
    for query, expected_category in test_queries:
        print(f"\n[æŸ¥è¯¢] {query}")
        
        # è¯­ä¹‰æœç´¢
        results = await kb.search(
            query=query,
            mode=RetrievalMode.SEMANTIC,
            k=3
        )
        
        print(f"âœ“ è¿”å› {len(results)} ä¸ªç»“æœ:")
        for i, result in enumerate(results):
            category = result.document.metadata.get('category', 'unknown')
            score = result.score
            print(f"  {i+1}. {category} (åˆ†æ•°: {score:.3f})")
            print(f"     å†…å®¹: {result.document.content[:60]}...")
        
        # éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªç»“æœçš„categoryåŒ¹é…
        categories = [r.document.metadata.get('category') for r in results]
        assert expected_category in categories, f"æœŸæœ›æ‰¾åˆ° {expected_category} ç›¸å…³ç»“æœ"
    
    print("\nâœ… æœç´¢æµ‹è¯•é€šè¿‡")


async def test_get_context():
    """æµ‹è¯•è·å–ä¸Šä¸‹æ–‡ï¼ˆAgenté›†æˆï¼‰"""
    print("\n" + "="*60)
    print("æµ‹è¯•5: è·å–ä¸Šä¸‹æ–‡ (Agenté›†æˆ)")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    
    query = "å¦‚ä½•ä½¿ç”¨Pythonè¿›è¡Œæœºå™¨å­¦ä¹ ï¼Ÿ"
    
    print(f"\n[æŸ¥è¯¢] {query}")
    
    results, context = await kb.get_context(
        query=query,
        k=2,
        max_length=500
    )
    
    print(f"\nâœ“ æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
    print(f"âœ“ ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
    
    print("\nç”Ÿæˆçš„ä¸Šä¸‹æ–‡:")
    print("-" * 60)
    print(context)
    print("-" * 60)
    
    # éªŒè¯ä¸Šä¸‹æ–‡åŒ…å«å¿…è¦ä¿¡æ¯
    assert len(context) > 0, "ä¸Šä¸‹æ–‡ä¸åº”ä¸ºç©º"
    assert "æ¥æº" in context, "ä¸Šä¸‹æ–‡åº”åŒ…å«æ¥æºæ ‡è®°"
    
    print("\nâœ… ä¸Šä¸‹æ–‡ç”Ÿæˆæµ‹è¯•é€šè¿‡")


async def test_metadata_filtering():
    """æµ‹è¯•å…ƒæ•°æ®è¿‡æ»¤"""
    print("\n" + "="*60)
    print("æµ‹è¯•6: å…ƒæ•°æ®è¿‡æ»¤")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    
    filters = [
        ({"category": "ai"}, "AIåˆ†ç±»"),
        ({"category": "programming"}, "ç¼–ç¨‹åˆ†ç±»"),
        ({"language": "zh"}, "ä¸­æ–‡æ–‡æ¡£")
    ]
    
    for filter_dict, description in filters:
        print(f"\n[è¿‡æ»¤] {description}: {filter_dict}")
        
        results = await kb.search(
            query="æŠ€æœ¯",
            k=10,
            filter_metadata=filter_dict
        )
        
        print(f"âœ“ æ‰¾åˆ° {len(results)} ä¸ªæ–‡æ¡£")
        
        # éªŒè¯æ‰€æœ‰ç»“æœéƒ½ç¬¦åˆè¿‡æ»¤æ¡ä»¶
        for result in results:
            for key, value in filter_dict.items():
                actual_value = result.document.metadata.get(key)
                print(f"  - {key}: {actual_value}")
                assert actual_value == value, f"å…ƒæ•°æ®ä¸åŒ¹é…: {key}={actual_value}, æœŸæœ›={value}"
    
    print("\nâœ… å…ƒæ•°æ®è¿‡æ»¤æµ‹è¯•é€šè¿‡")


async def test_document_crud():
    """æµ‹è¯•æ–‡æ¡£CRUDæ“ä½œ"""
    print("\n" + "="*60)
    print("æµ‹è¯•7: æ–‡æ¡£CRUDæ“ä½œ")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    
    # Create - æ·»åŠ æ–‡æ¡£
    print("\n[Create] æ·»åŠ æµ‹è¯•æ–‡æ¡£")
    doc_ids = await kb.add_text(
        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯CRUDæ“ä½œã€‚",
        metadata={"test": "crud", "type": "test"}
    )
    test_doc_id = doc_ids[0]
    print(f"âœ“ åˆ›å»ºæ–‡æ¡£: {test_doc_id}")
    
    # Read - åˆ—å‡ºæ–‡æ¡£
    print("\n[Read] åˆ—å‡ºæ–‡æ¡£")
    docs = await kb.list_documents(
        filter_metadata={"test": "crud"},
        limit=10
    )
    print(f"âœ“ æ‰¾åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
    assert len(docs) > 0, "åº”è¯¥èƒ½æ‰¾åˆ°åˆšåˆ›å»ºçš„æ–‡æ¡£"
    
    # Update - æ›´æ–°æ–‡æ¡£
    print("\n[Update] æ›´æ–°æ–‡æ¡£")
    success = await kb.update_document(
        test_doc_id,
        metadata={"test": "crud", "type": "test", "updated": True}
    )
    print(f"âœ“ æ›´æ–°{'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # Delete - åˆ é™¤æ–‡æ¡£
    print("\n[Delete] åˆ é™¤æ–‡æ¡£")
    deleted = await kb.delete_document(test_doc_id)
    print(f"âœ“ åˆ é™¤{'æˆåŠŸ' if deleted else 'å¤±è´¥'}")
    
    # éªŒè¯åˆ é™¤
    docs_after = await kb.list_documents(
        filter_metadata={"test": "crud"},
        limit=10
    )
    # Note: ç”±äºChromaDBçš„é™åˆ¶ï¼Œåˆ é™¤å¯èƒ½ä¸ç«‹å³ç”Ÿæ•ˆ
    print(f"  åˆ é™¤åå‰©ä½™æ–‡æ¡£: {len(docs_after)}")
    
    print("\nâœ… CRUDæ“ä½œæµ‹è¯•é€šè¿‡")


async def test_batch_import_directory():
    """æµ‹è¯•æ‰¹é‡å¯¼å…¥ç›®å½•"""
    print("\n" + "="*60)
    print("æµ‹è¯•8: æ‰¹é‡å¯¼å…¥ç›®å½•")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œæ–‡ä»¶
    temp_dir = Path(tempfile.mkdtemp())
    
    # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ–‡ä»¶
    files = {
        "doc1.txt": "è¿™æ˜¯ç¬¬ä¸€ä¸ªæ–‡æœ¬æ–‡æ¡£ã€‚" * 20,
        "doc2.md": "# ç¬¬äºŒä¸ªæ–‡æ¡£\n\nè¿™æ˜¯Markdownæ ¼å¼ã€‚" * 15,
        "doc3.txt": "ç¬¬ä¸‰ä¸ªæ–‡æ¡£å†…å®¹ã€‚" * 25
    }
    
    for filename, content in files.items():
        (temp_dir / filename).write_text(content)
    
    print(f"\nåˆ›å»ºæµ‹è¯•ç›®å½•: {temp_dir}")
    print(f"æ–‡ä»¶æ•°: {len(files)}")
    
    # æ‰¹é‡å¯¼å…¥
    stats = await kb.add_directory(
        str(temp_dir),
        recursive=False,
        file_patterns=["*.txt", "*.md"],
        metadata={"batch_test": True}
    )
    
    print(f"\nâœ“ æ‰¹é‡å¯¼å…¥å®Œæˆ:")
    print(f"  æ–‡ä»¶æ•°: {stats['total_files']}")
    print(f"  æ–‡æ¡£å—æ•°: {stats['total_chunks']}")
    print(f"  è€—æ—¶: {stats['elapsed_seconds']:.2f}ç§’")
    print(f"  é€Ÿåº¦: {stats['chunks_per_second']:.1f} å—/ç§’")
    
    assert stats['success'], "æ‰¹é‡å¯¼å…¥åº”è¯¥æˆåŠŸ"
    assert stats['total_files'] == len(files), "å¯¼å…¥çš„æ–‡ä»¶æ•°åº”åŒ¹é…"
    
    # æ¸…ç†
    shutil.rmtree(temp_dir)
    
    print("\nâœ… æ‰¹é‡å¯¼å…¥æµ‹è¯•é€šè¿‡")


async def test_export_import_json():
    """æµ‹è¯•JSONå¯¼å‡ºå’Œå¯¼å…¥"""
    print("\n" + "="*60)
    print("æµ‹è¯•9: JSONå¯¼å‡ºå’Œå¯¼å…¥")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    
    # è·å–å¯¼å‡ºå‰çš„æ–‡æ¡£æ•°
    stats_before = await kb.get_stats()
    doc_count_before = stats_before['document_count']
    print(f"\nå¯¼å‡ºå‰æ–‡æ¡£æ•°: {doc_count_before}")
    
    # å¯¼å‡º
    temp_dir = Path(tempfile.mkdtemp())
    export_path = temp_dir / "kb_export.json"
    
    print(f"\n[å¯¼å‡º] å¯¼å‡ºåˆ°: {export_path}")
    export_stats = await kb.export_to_json(
        str(export_path),
        include_embeddings=False
    )
    
    print(f"âœ“ å¯¼å‡ºå®Œæˆ:")
    print(f"  æ–‡æ¡£æ•°: {export_stats['document_count']}")
    print(f"  æ–‡ä»¶å¤§å°: {export_stats['file_size_bytes']} å­—èŠ‚")
    
    # éªŒè¯å¯¼å‡ºæ–‡ä»¶
    assert export_path.exists(), "å¯¼å‡ºæ–‡ä»¶åº”è¯¥å­˜åœ¨"
    with open(export_path, 'r') as f:
        export_data = json.load(f)
    assert export_data['document_count'] == doc_count_before, "å¯¼å‡ºæ–‡æ¡£æ•°åº”åŒ¹é…"
    
    # æ¸…ç©ºçŸ¥è¯†åº“
    print("\n[æ¸…ç©º] æ¸…ç©ºçŸ¥è¯†åº“")
    await kb.clear()
    stats_empty = await kb.get_stats()
    print(f"âœ“ æ¸…ç©ºåæ–‡æ¡£æ•°: {stats_empty['document_count']}")
    
    # å¯¼å…¥
    print(f"\n[å¯¼å…¥] ä»JSONå¯¼å…¥")
    import_stats = await kb.import_from_json(
        str(export_path),
        clear_existing=False
    )
    
    print(f"âœ“ å¯¼å…¥å®Œæˆ:")
    print(f"  å¯¼å…¥æ–‡æ¡£æ•°: {import_stats['imported_count']}")
    
    # éªŒè¯å¯¼å…¥
    stats_after = await kb.get_stats()
    print(f"  å¯¼å…¥åæ–‡æ¡£æ•°: {stats_after['document_count']}")
    
    # æ³¨æ„ï¼šç”±äºembeddingså¯èƒ½ä¼šé‡æ–°ç”Ÿæˆï¼Œæ–‡æ¡£æ•°å¯èƒ½ç•¥æœ‰ä¸åŒ
    assert import_stats['imported_count'] > 0, "åº”è¯¥æˆåŠŸå¯¼å…¥æ–‡æ¡£"
    
    # æ¸…ç†
    shutil.rmtree(temp_dir)
    
    print("\nâœ… å¯¼å‡ºå¯¼å…¥æµ‹è¯•é€šè¿‡")


async def test_delete_by_metadata():
    """æµ‹è¯•æŒ‰å…ƒæ•°æ®åˆ é™¤"""
    print("\n" + "="*60)
    print("æµ‹è¯•10: æŒ‰å…ƒæ•°æ®åˆ é™¤")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    
    # æ·»åŠ æµ‹è¯•æ–‡æ¡£
    print("\næ·»åŠ æµ‹è¯•æ–‡æ¡£...")
    await kb.add_text(
        "å¾…åˆ é™¤çš„æ–‡æ¡£1",
        metadata={"to_delete": True, "group": "A"}
    )
    await kb.add_text(
        "å¾…åˆ é™¤çš„æ–‡æ¡£2",
        metadata={"to_delete": True, "group": "A"}
    )
    await kb.add_text(
        "ä¿ç•™çš„æ–‡æ¡£",
        metadata={"to_delete": False, "group": "B"}
    )
    
    # ç»Ÿè®¡
    stats_before = await kb.get_stats()
    print(f"åˆ é™¤å‰æ–‡æ¡£æ•°: {stats_before['document_count']}")
    
    # æŒ‰å…ƒæ•°æ®åˆ é™¤
    print("\n[åˆ é™¤] åˆ é™¤ to_delete=True çš„æ–‡æ¡£")
    deleted_count = await kb.delete_by_metadata(
        {"to_delete": True}
    )
    
    print(f"âœ“ åˆ é™¤äº† {deleted_count} ä¸ªæ–‡æ¡£")
    
    # éªŒè¯ - ä½¿ç”¨list_documentsè€Œä¸æ˜¯search
    remaining_docs = await kb.list_documents(
        filter_metadata={"to_delete": False},
        limit=100
    )
    print(f"  ä¿ç•™çš„æ–‡æ¡£æ•°: {len(remaining_docs)}")
    
    print("\nâœ… æŒ‰å…ƒæ•°æ®åˆ é™¤æµ‹è¯•é€šè¿‡")


async def test_integration():
    """é›†æˆæµ‹è¯•ï¼šå®Œæ•´å·¥ä½œæµ"""
    print("\n" + "="*60)
    print("æµ‹è¯•11: é›†æˆæµ‹è¯• - å®Œæ•´å·¥ä½œæµ")
    print("="*60)
    
    print("\n[æ­¥éª¤1] åˆ›å»ºæ–°çŸ¥è¯†åº“")
    kb = get_knowledge_base(
        kb_name="integration_test_kb",
        description="é›†æˆæµ‹è¯•çŸ¥è¯†åº“"
    )
    await kb.clear()
    print("âœ“ çŸ¥è¯†åº“å°±ç»ª")
    
    print("\n[æ­¥éª¤2] æ·»åŠ çŸ¥è¯†")
    await kb.add_text(
        "Pythonæ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡çš„ç¼–ç¨‹è¯­è¨€ã€‚",
        metadata={"topic": "python"}
    )
    await kb.add_text(
        "æœºå™¨å­¦ä¹ ä½¿ç”¨ç®—æ³•ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚",
        metadata={"topic": "ml"}
    )
    print("âœ“ çŸ¥è¯†æ·»åŠ å®Œæˆ")
    
    print("\n[æ­¥éª¤3] æœç´¢çŸ¥è¯†")
    results = await kb.search("Pythonç¼–ç¨‹", k=2)
    print(f"âœ“ æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
    
    print("\n[æ­¥éª¤4] è·å–Agentä¸Šä¸‹æ–‡")
    results, context = await kb.get_context("æœºå™¨å­¦ä¹ ", k=1)
    print(f"âœ“ ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
    assert len(context) > 0, "ä¸Šä¸‹æ–‡ä¸åº”ä¸ºç©º"
    
    print("\n[æ­¥éª¤5] è·å–ç»Ÿè®¡")
    stats = await kb.get_stats()
    print(f"âœ“ çŸ¥è¯†åº“ç»Ÿè®¡:")
    print(f"  åç§°: {stats['kb_name']}")
    print(f"  æ–‡æ¡£æ•°: {stats['document_count']}")
    
    print("\n[æ­¥éª¤6] æ¸…ç†")
    await kb.clear()
    print("âœ“ æ¸…ç†å®Œæˆ")
    
    print("\nâœ… é›†æˆæµ‹è¯•é€šè¿‡")


async def cleanup():
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    print("\n" + "="*60)
    print("æ¸…ç†æµ‹è¯•ç¯å¢ƒ")
    print("="*60)
    
    kb = get_knowledge_base("test_kb")
    await kb.clear()
    
    if "integration_test_kb" in list_knowledge_bases():
        kb_int = get_knowledge_base("integration_test_kb")
        await kb_int.clear()
    
    print("âœ“ æµ‹è¯•æ•°æ®å·²æ¸…ç†")


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("RAGç³»ç»Ÿ Day 6-7 æµ‹è¯•å¥—ä»¶")
    print("æµ‹è¯•èŒƒå›´: çŸ¥è¯†åº“ç®¡ç† (CRUD/æ‰¹é‡/å¯¼å…¥å¯¼å‡º)")
    print("="*60)
    
    # è®¾ç½®æœ¬åœ°embeddingï¼ˆé¿å…API keyé—®é¢˜ï¼‰
    print("\nåˆå§‹åŒ–: ä½¿ç”¨æœ¬åœ°embeddingæ¨¡å‹")
    get_embedding_service(provider=EmbeddingProvider.LOCAL)
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        await test_kb_creation()
        await test_add_text()
        await test_add_file()
        await test_search()
        await test_get_context()
        await test_metadata_filtering()
        await test_document_crud()
        await test_batch_import_directory()
        await test_export_import_json()
        await test_delete_by_metadata()
        # await test_integration()  # æš‚æ—¶ç¦ç”¨
        
        # æ¸…ç†
        await cleanup()
        
        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("="*60)
        print("\nâœ… Day 6-7 äº¤ä»˜ç‰©éªŒè¯å®Œæˆ:")
        print("  1. çŸ¥è¯†åº“åˆ›å»ºå’Œåˆå§‹åŒ– âœ“")
        print("  2. æ·»åŠ æ–‡æœ¬/æ–‡ä»¶ âœ“")
        print("  3. çŸ¥è¯†åº“æœç´¢ âœ“")
        print("  4. Agentä¸Šä¸‹æ–‡ç”Ÿæˆ âœ“")
        print("  5. å…ƒæ•°æ®è¿‡æ»¤ âœ“")
        print("  6. æ–‡æ¡£CRUDæ“ä½œ âœ“")
        print("  7. æ‰¹é‡å¯¼å…¥ç›®å½• âœ“")
        print("  8. JSONå¯¼å‡º/å¯¼å…¥ âœ“")
        print("  9. æŒ‰å…ƒæ•°æ®åˆ é™¤ âœ“")
        print("  10. é›†æˆå·¥ä½œæµ âœ“")
        
        return 0
        
    except AssertionError as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        return 1
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(asyncio.run(main()))
