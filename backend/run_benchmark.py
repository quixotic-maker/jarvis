"""åŸºå‡†æµ‹è¯•æ‰§è¡Œè„šæœ¬ - éªŒè¯Promptç³»ç»Ÿæå‡æ•ˆæœ

è¿è¡Œå¯¹æ¯”æµ‹è¯•ï¼Œè¯„ä¼°æ–°Promptç³»ç»Ÿç›¸å¯¹äºæ—§ç³»ç»Ÿçš„æ”¹è¿›æ•ˆæœ
"""
import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import statistics

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from benchmark_dataset import BenchmarkDataset, EvaluationCriteria, EXPECTED_IMPROVEMENTS
from app.agents.coordinator_agent import CoordinatorAgent
from app.agents.schedule_agent import ScheduleAgent
from app.agents.task_agent import TaskAgent
from app.agents.code_agent import CodeAgent
from app.db.database import SessionLocal


class BenchmarkRunner:
    """åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.results = {}
        self.dataset = BenchmarkDataset()
        
    async def run_coordinator_tests(self):
        """è¿è¡ŒCoordinatoræµ‹è¯•"""
        print("\n" + "="*60)
        print("æµ‹è¯•1: Coordinatoræ„å›¾è¯†åˆ«å‡†ç¡®ç‡")
        print("="*60 + "\n")
        
        coordinator = CoordinatorAgent()
        tests = self.dataset.COORDINATOR_TESTS
        
        correct_agent = 0
        correct_params = 0
        total = len(tests)
        
        for i, test in enumerate(tests, 1):
            print(f"[{i}/{total}] æµ‹è¯•: {test['input']}")
            
            result = await coordinator.execute({
                "user_input": test["input"],
                "context": {},
                "user_profile": {}
            })
            
            # æ£€æŸ¥Agenté€‰æ‹©
            assigned_agent = result.get("assigned_agent")
            expected_agent = test["expected_agent"]
            
            if assigned_agent == expected_agent:
                correct_agent += 1
                print(f"  âœ… Agentæ­£ç¡®: {assigned_agent}")
            else:
                print(f"  âŒ Agenté”™è¯¯: {assigned_agent} (æœŸæœ›: {expected_agent})")
            
            # æ£€æŸ¥å‚æ•°æå–ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
            params = result.get("parameters", {})
            if params:
                correct_params += 1
                print(f"  âœ… å‚æ•°æå–: {params}")
            else:
                print(f"  âš ï¸  å‚æ•°ç¼ºå¤±")
            
            print()
        
        agent_accuracy = correct_agent / total
        param_accuracy = correct_params / total
        
        print(f"\nğŸ“Š Coordinatoræµ‹è¯•ç»“æœ:")
        print(f"  Agenté€‰æ‹©å‡†ç¡®ç‡: {agent_accuracy:.1%} ({correct_agent}/{total})")
        print(f"  å‚æ•°æå–å‡†ç¡®ç‡: {param_accuracy:.1%} ({correct_params}/{total})")
        
        return {
            "agent_accuracy": agent_accuracy,
            "param_accuracy": param_accuracy,
            "total_tests": total
        }
    
    async def run_schedule_tests(self):
        """è¿è¡ŒScheduleæµ‹è¯•"""
        print("\n" + "="*60)
        print("æµ‹è¯•2: ScheduleAgentæ—¶é—´æå–å‡†ç¡®ç‡")
        print("="*60 + "\n")
        
        schedule_agent = ScheduleAgent()
        db = SessionLocal()
        tests = self.dataset.SCHEDULE_TESTS
        
        successful = 0
        total = len(tests)
        
        for i, test in enumerate(tests, 1):
            print(f"[{i}/{total}] æµ‹è¯•: {test['input']}")
            
            try:
                result = await schedule_agent.execute({
                    "action": "create",
                    "user_input": test["input"],
                    "db": db
                })
                
                if result.get("success"):
                    successful += 1
                    schedule = result.get("schedule", {})
                    print(f"  âœ… æ—¥ç¨‹åˆ›å»ºæˆåŠŸ")
                    print(f"     æ ‡é¢˜: {schedule.get('title')}")
                    print(f"     æ—¶é—´: {schedule.get('start_time')}")
                else:
                    print(f"  âŒ åˆ›å»ºå¤±è´¥: {result.get('error')}")
            except Exception as e:
                print(f"  âŒ å¼‚å¸¸: {str(e)}")
            
            print()
        
        db.close()
        
        accuracy = successful / total
        print(f"\nğŸ“Š Scheduleæµ‹è¯•ç»“æœ:")
        print(f"  æ—¥ç¨‹åˆ›å»ºæˆåŠŸç‡: {accuracy:.1%} ({successful}/{total})")
        
        return {
            "success_rate": accuracy,
            "total_tests": total
        }
    
    async def run_task_tests(self):
        """è¿è¡ŒTaskæµ‹è¯•"""
        print("\n" + "="*60)
        print("æµ‹è¯•3: TaskAgentä¼˜å…ˆçº§åˆ¤æ–­å‡†ç¡®ç‡")
        print("="*60 + "\n")
        
        task_agent = TaskAgent()
        db = SessionLocal()
        tests = self.dataset.TASK_TESTS
        
        correct_priority = 0
        successful = 0
        total = len(tests)
        
        for i, test in enumerate(tests, 1):
            print(f"[{i}/{total}] æµ‹è¯•: {test['input']}")
            
            try:
                result = await task_agent.execute({
                    "action": "create",
                    "user_input": test["input"],
                    "db": db
                })
                
                if result.get("success"):
                    successful += 1
                    task = result.get("task", {})
                    priority = task.get("priority")
                    expected_priority = test["expected"].get("priority")
                    
                    if expected_priority and priority == expected_priority:
                        correct_priority += 1
                        print(f"  âœ… ä¼˜å…ˆçº§æ­£ç¡®: {priority}")
                    else:
                        print(f"  âš ï¸  ä¼˜å…ˆçº§: {priority} (æœŸæœ›: {expected_priority})")
                    
                    print(f"     æ ‡é¢˜: {task.get('title')}")
                else:
                    print(f"  âŒ åˆ›å»ºå¤±è´¥: {result.get('error')}")
            except Exception as e:
                print(f"  âŒ å¼‚å¸¸: {str(e)}")
            
            print()
        
        db.close()
        
        priority_accuracy = correct_priority / total if total > 0 else 0
        success_rate = successful / total
        
        print(f"\nğŸ“Š Taskæµ‹è¯•ç»“æœ:")
        print(f"  ä¼˜å…ˆçº§åˆ¤æ–­å‡†ç¡®ç‡: {priority_accuracy:.1%} ({correct_priority}/{total})")
        print(f"  ä»»åŠ¡åˆ›å»ºæˆåŠŸç‡: {success_rate:.1%} ({successful}/{total})")
        
        return {
            "priority_accuracy": priority_accuracy,
            "success_rate": success_rate,
            "total_tests": total
        }
    
    async def run_code_tests(self):
        """è¿è¡ŒCodeæµ‹è¯•"""
        print("\n" + "="*60)
        print("æµ‹è¯•4: CodeAgentä»£ç ç”Ÿæˆè´¨é‡")
        print("="*60 + "\n")
        
        code_agent = CodeAgent()
        tests = self.dataset.CODE_TESTS
        
        successful = 0
        total = len(tests)
        
        for i, test in enumerate(tests, 1):
            print(f"[{i}/{total}] æµ‹è¯•: {test['input']}")
            
            try:
                result = await code_agent.execute({
                    "user_input": test["input"],
                    "parameters": {"action": "generate", "language": "Python"}
                })
                
                if result.get("success"):
                    successful += 1
                    code = result.get("code", "")
                    print(f"  âœ… ä»£ç ç”ŸæˆæˆåŠŸ")
                    print(f"     é•¿åº¦: {len(code)}å­—ç¬¦")
                    print(f"     å‰100å­—ç¬¦: {code[:100]}...")
                else:
                    print(f"  âŒ ç”Ÿæˆå¤±è´¥: {result.get('error')}")
            except Exception as e:
                print(f"  âŒ å¼‚å¸¸: {str(e)}")
            
            print()
        
        success_rate = successful / total
        print(f"\nğŸ“Š Codeæµ‹è¯•ç»“æœ:")
        print(f"  ä»£ç ç”ŸæˆæˆåŠŸç‡: {success_rate:.1%} ({successful}/{total})")
        
        return {
            "success_rate": success_rate,
            "total_tests": total
        }
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("\n" + "ğŸš€ " + "="*56 + " ğŸš€")
        print("ğŸš€    å¼€å§‹åŸºå‡†æµ‹è¯• - Promptç³»ç»Ÿæ•ˆæœéªŒè¯    ğŸš€")
        print("ğŸš€ " + "="*56 + " ğŸš€\n")
        
        print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•æ•°æ®é›†: {self.dataset.get_total_test_count()}ä¸ªç”¨ä¾‹")
        
        # è¿è¡Œå„é¡¹æµ‹è¯•
        results = {}
        results["coordinator"] = await self.run_coordinator_tests()
        results["schedule"] = await self.run_schedule_tests()
        results["task"] = await self.run_task_tests()
        results["code"] = await self.run_code_tests()
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self.generate_report(results)
        
        return results
    
    def generate_report(self, results: Dict):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š åŸºå‡†æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("="*60 + "\n")
        
        print("1ï¸âƒ£  Coordinator Agent")
        coord_result = results["coordinator"]
        coord_expected = EXPECTED_IMPROVEMENTS["Coordinator"]
        print(f"   Agenté€‰æ‹©å‡†ç¡®ç‡: {coord_result['agent_accuracy']:.1%}")
        print(f"   - åŸºçº¿: {coord_expected['agent_accuracy']['baseline']:.0%}")
        print(f"   - ç›®æ ‡: {coord_expected['agent_accuracy']['target']:.0%}")
        print(f"   - å®é™…æå‡: {(coord_result['agent_accuracy'] - coord_expected['agent_accuracy']['baseline']) / coord_expected['agent_accuracy']['baseline']:.1%}")
        
        print(f"\n   å‚æ•°æå–å‡†ç¡®ç‡: {coord_result['param_accuracy']:.1%}")
        print(f"   - åŸºçº¿: {coord_expected['param_extraction']['baseline']:.0%}")
        print(f"   - ç›®æ ‡: {coord_expected['param_extraction']['target']:.0%}")
        
        print(f"\n\n2ï¸âƒ£  Schedule Agent")
        sched_result = results["schedule"]
        sched_expected = EXPECTED_IMPROVEMENTS["Schedule"]
        print(f"   æ—¥ç¨‹åˆ›å»ºæˆåŠŸç‡: {sched_result['success_rate']:.1%}")
        print(f"   - åŸºçº¿: {sched_expected['time_extraction']['baseline']:.0%}")
        print(f"   - ç›®æ ‡: {sched_expected['time_extraction']['target']:.0%}")
        
        print(f"\n\n3ï¸âƒ£  Task Agent")
        task_result = results["task"]
        task_expected = EXPECTED_IMPROVEMENTS["Task"]
        print(f"   ä¼˜å…ˆçº§åˆ¤æ–­å‡†ç¡®ç‡: {task_result['priority_accuracy']:.1%}")
        print(f"   - åŸºçº¿: {task_expected['priority_judgment']['baseline']:.0%}")
        print(f"   - ç›®æ ‡: {task_expected['priority_judgment']['target']:.0%}")
        
        print(f"\n\n4ï¸âƒ£  Code Agent")
        code_result = results["code"]
        code_expected = EXPECTED_IMPROVEMENTS["Code"]
        print(f"   ä»£ç ç”ŸæˆæˆåŠŸç‡: {code_result['success_rate']:.1%}")
        print(f"   - åŸºçº¿: {code_expected['code_quality']['baseline']:.0%}")
        print(f"   - ç›®æ ‡: {code_expected['code_quality']['target']:.0%}")
        
        print("\n\n" + "="*60)
        print("ğŸ‰ æ ¸å¿ƒå‘ç°")
        print("="*60)
        print("\nâœ¨ Promptç³»ç»Ÿé›†æˆæˆåŠŸï¼")
        print("\nğŸ“ˆ ä¸»è¦æ”¹è¿›:")
        print("   1. ç»Ÿä¸€çš„ä¸“ä¸šç³»ç»Ÿæç¤ºè¯ï¼ˆ21ä¸ªAgentï¼‰")
        print("   2. Few-shotç¤ºä¾‹å­¦ä¹ æœºåˆ¶")
        print("   3. Chain-of-Thoughtæ¨ç†æ”¯æŒ")
        print("   4. åŠ¨æ€Promptç»„è£…èƒ½åŠ›")
        
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("   1. æ”¶é›†æ›´å¤šçœŸå®ç”¨æˆ·æ•°æ®è¿›è¡Œæµ‹è¯•")
        print("   2. æ ¹æ®åé¦ˆä¼˜åŒ–Promptæ¨¡æ¿")
        print("   3. ç»§ç»­é›†æˆå‰©ä½™Agent")
        print("   4. å»ºç«‹æŒç»­ç›‘æ§å’Œä¼˜åŒ–æœºåˆ¶")
        
        print("\n" + "="*60 + "\n")


async def main():
    """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
    runner = BenchmarkRunner()
    await runner.run_all_tests()


if __name__ == "__main__":
    asyncio.run(main())
